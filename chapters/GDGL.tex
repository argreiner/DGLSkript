\chapter{Gewöhnliche Differentialgleichungen}
Die allgemeine Form einer Differentialgleichung sei folgendermaßen gegeben
\begin{equation}
\frac{d\mbf{y}(t)}{dt}=f(t,\mbf{y}(t))
  \label{eq:DGLallgemein}
\end{equation}
Anstatt den Differentialoperator $\frac{d}{dt}$ auszuschreiben, benutzen wir
für (\ref{eq:DGLallgemein}) auch die Schreibweise
$\dot{\mbf{y}}=f(t,\mbf{y}(t))$.  Dabei bezeichne $t$ die unabhängige
Veränderliche, $f(t,\mbf{y})$ eine bekannte Funktion und $\mbf{y}$ einen Vektor
gesuchter Funktionen $y_i(t)$.  Wir betrachten verschiedene
Differentialgleichungen, benennen deren Eigenschaften und erkennen die
verschiedenen Typen:
\begin{enumerate}
	\item Linear und nichtlinear, wie z.B.\
	  \[m\ddot{x}(t)+c\dot{x}(t)+kx(t)=f(t),\]
	  eine lineare inhomogene Differentialgleichung 2. Ordnung,
	  die den gedämpften und getriebenen harmonischen Oszillator bechreibt, während
	\[\frac{d^2x(t)}{dt^2}+\mu(x(t)^2-1)\frac{dx(t)}{dt}+x(t)= 0 \]
	  eine nichtlineare Bewegungsgleichung für $x(t)$ ist. Sie beschreibt den so genannten
	  van der Pol Oszillator.  
	\item erste, zweite und höhere Ordnung 
	  \[ \dot{x}(t)+\frac{1}{\tau}x=h(t)\] 
	  beschreibt einen getriebenen Relaxator, den wir gleich näher
	  betrachten werden. Die rechte Seite dieser Gleichung bezeichnen wir
	  als Inhomogenität.
		%- in der Systemtheorievbezeichneten wir damit ein
		%Verzögerungsglied 1.\ Ordnung ($\mbox{PT}_1$-Glied). 
	  Ein Beispiel für eine Gleichung 2.\ Ordnung ist entweder der
	  van der Pol Oszillator oder der Harmonische Oszillator (siehe
	  oben). 
		% Finde ein entsprechendes elementares zeitinvariantes
		% \"Ubertragungsglied (Hinweis: Skript Systemtheorie)!
	\item System von DEs  1.\ Ordnung 
		\begin{eqnarray*}
			\frac{dx}{dt} &=& x(\alpha - \beta y) \\
			\frac{dy}{dt} &=& - y(\gamma - \delta x) 
		\end{eqnarray*}
		die bekannte Räuber-Beute-Gleichungen oder auch Lotka-Volterra-Gleichungen.
	\item Transformation von Gleichungen höherer Ordnung auf ein System von Gleichungen 1.\ Ordnung.
		Gegeben der gedämpfte Harmonische Oszillator
		\[m\ddot{x}(t)+c\dot{x}(t)+kx=f(t)\]
		ersetze $\dot{x} = y$ und wir erhalten zwei Gleichungen erster
		Ordnung anstatt der ursprünglichen Gleichung zweiter Ordnung, nämlich 
		\begin{eqnarray*}
		  \dot{x} &=& y\\
	 	  m\dot{y}&=&-cy-kx+f(t)
	       \end{eqnarray*}
	     \item Der Grad einer Differentialgleichung ist der Exponent der Potenz der höchsten vorkommenden Ableitung.
\end{enumerate}
Bei all diesen Differentialgleichungen sind wir immer an einer Lösung für
einen bestimmten Anfangswert interessiert, also z.B.\ $x(t=0)=x_0$ etc. Um ein
System von Gleichungen lösen zu können müssen wir für jede Variable
Anfangsbedingungen angeben. Da aus einer Gleichung n-ter Ordnung ein System mit
n Variablen wird, schließen wir daraus, dass wir für eine solche
Differentialgleichung ebenfalls n Anfangsbedingungen brauchen.
%
\section{Allgemeine analytische Lösung}\label{sec:analyticsolu}
Wenn wir gewöhnliche Differentialgleichungen analytisch lösen wollen so ist
dies nicht in allen Fällen möglich. Betrachten wir das Anfangswertproblem
für die gewöhnliche Differentialgleichung
\[ \dot{y}(t) = f\left(t,y(t)\right)\]
mit $y(t_0)=y_0$. Die Funktion $f(t,y(t))$ sei stetig im Bereich der
$(t,y)$-Ebene, gegeben durch $t_0-a\le t\le t_0+a$ und $y_0 -b\le y\le y_0+b$.
Der Nachweis der Existenz und Eindeutigkeit einer Lösung ist in diesem Falle
nicht immer möglich. Wir wollen eine alternative Vorgehensweise vorschlagen.

Wir nehmen an $y(t)$ sei eine Lösung des Anfangswertproblems und es gelte
$y_0 -b\le y\le y_0+b$ für $t\in [t_0-\alpha,t_0+\alpha]$ mit $0<\alpha\le a$,
so ergibt sich aus obiger Voraussetzung für $f(t,y(t))$, dass die Lösung im
Intervall $[t_0-\alpha,t_0+\alpha]$ eine stetige Ableitungsfunktion
$\dot{y}(t)$ besitzt.  Also lässt sich die Lösungsfunktion dort als
Stammfunktion dieser Ableitung darstellen 
\begin{equation}\label{eq:Integral}
  y(t)=y_0+\int_{t_0}^t f(s,y(s))ds 
\end{equation}
Wobei $t_0-a\le t\le t_0+a$ gilt. Dies bedeutet aber, dass jede Lösung des
obigen Anfangswertproblems auch Lösung der Integralgleichung
(\ref{eq:Integral}) ist. Umgekehrt ist auch jede Lösung von
(\ref{eq:Integral}) Lösung des Anfangswertproblems.
\section{Lösung durch Iteration}
Schreiben wir die Integralgleichung (\ref{eq:Integral}) in der Form
\begin{equation}
  y(t)=\Phi[y]\mbox{ mit } \Phi[y]=y_0+\int_{t_0}^t f(s,y(s))ds
  \label{eq:Iteration}
\end{equation}
\begin{note}{Funktional statt Integral}
  In (\ref{eq:Iteration}) schreiben wir absichtlich $\Phi[y]$, d.h. das $y$ in
  eckigen Klammern geschrieben, weil $\Phi[y]$ ein Funktional ist. Es hängt
  nicht nur vom Zeitpunkt $t$ in der unabhängigen Variablen ab, sondern vom
  gesamten Verlauf der Funktion $y(t)$. Wir werden im Kapitel \ref{chap:Variationsrechnung}
  Variationsrechnung darauf zurückkommen. 
\end{note}
Wir sehen, dass die Lösung von (\ref{eq:Iteration}) wie der Fixpunkt $y^*$ der
Gleichung \[ y=\varphi(y)\] zu bestimmen ist. Hierbei gehen wir iterativ vor.
Wir starten bei einem Rohwert $y_0$ und berechnen sukzessiv nach der Vorschrift
\[ y_{n+1} = \varphi(y_n) \] 
Jetzt müssen wir nur noch zeigen, dass die Iteration konvergiert. Dies ist für
die Lösung der Integralgleichung (\ref{eq:Integral}) der Fall, wenn die
Abbildung $\phi$ kontrahierend ist.
\begin{example}{Iterative Lösung}
Gegeben die Differentialgleichung
\[
  \frac{d}{dt}y(t)=-\alpha y(t) 
\]
Zeige, dass die iterative Lösung im Limes mit der analytischen Lösung
übereinstimmt.
\end{example}
\section{Gleichungen erster Ordnung und ersten Grades}
Wir gehen von einer Differentialgleichung erster Ordnung und ersten Grades 
\[   p(t,y)+q(t,y)\dot{y}(t)=0 \]
aus, die sich in der Form
\begin{equation}\label{eq:DGLVarKoeff}
  p(t,y)dt+q(t,y)dy=0
\end{equation}
schreiben lässt. Wobei $p(t,y)$ und $q(t,y)$ den gemeinsamen Definitionsbereich
$a\le t\le b$ und $\alpha\le y\le\beta$ haben
\begin{example}{Umformung}
  \[\frac{dy}{dt}+\frac{y+t}{y-t}=0\]
  kann mit $p(t,y)=y+t$ und $q(t,y)=y-t$ auf die Form
  \[(y+t)dt+(y-t)dy=0\]
  gebracht werden.
\end{example}
\subsection{Separierbare Differentialgleichungen}
Den Spezialfall
\begin{equation}
  p(t)+q(y)\dot{y}(t)=0
  \label{eq:getrennteVar}
\end{equation}
nennen wir eine {\it Differentialgleichung mit getrennten Variablen}.

Existieren für $p$ und $q$ im angegebenen Definitionsbereich die Integrale $P$
und $Q$, mit $\dot{P}(t)=p(t)$ und $\frac{dQ(y)}{dy}=q(y)$ so ist
\[\mu(t,y)=P(t)+Q(y)=const.\]
denn es ist wegen (\ref{eq:getrennteVar})
\[ d\mu=\dot{P}(t)dt+\frac{dQ(y)}{dy}dy=p(t)dt+q(y)dy=0\]
\begin{example}{Separierbare Differentialgleichung}
a) $t\dot{y}=y$ und b) $y\dot{y}=-t$
\begin{description}
  \item[a)] Für $t\ne 0$ und $y\ne =0$ erhalten wir 
  \[\frac{dy}{y}=\frac{dt}{t}\]
\item[b)] $ydy=-tdt$
\end{description}
\end{example}
\subsection{Exakte Differentialgleichungen}
Ist $p(t,y)dt+q(t,y)dy$ das vollständige Differential einer Funktion
$\mu(y,t)$, d.h.\ ist 
\[d\mu(y,t)=\frac{\partial\mu(y,t)}{\partial t}dt+
\frac{\partial\mu(y,t)}{\partial y}dy=p(t,y)dt+q(t,y)dy\]
dann nennen wir (\ref{eq:DGLVarKoeff}) eine {\it exakte Differentialgleichung}
und $\mu(y,t)=const.$ ist eine allgemeine Lösung. Damit (\ref{eq:DGLVarKoeff})
eine exakte Differentialgleichung ist, muss die Bedingung
\begin{equation}
  \frac{\partial p(t,y)}{\partial y}=  \frac{\partial q(t,y)}{\partial t}
  \label{eq:BedExaktheit}
\end{equation}
erfüllt sein.

Nicht immer sieht man allerdings der Differentialgleichung die Exaktheit an.
Selbst wenn wir sehen, dass (\ref{eq:DGLVarKoeff}) keine exakte
Differentialgleichung ist, da (\ref{eq:BedExaktheit}) nicht erfüllt ist, gibt
es aber unter Umständen die Möglichkeit eine Funktion zu finden, mit der man
die Gleichung multiplizieren kann, sodass diese exakt wird. 
\begin{note}{Integrierender Faktor}
  $3ydt+2tdy=0$ ist keine exakte Differentialgleichung. Multipliziert man aber
  $\Xi(y,t)=t^2y$ mit dieser Gleichung, so erhält man $3t^2y^2dt+2t^3ydy=0$.
  Dies kann man als vollständiges Differential der unktion $\mu(y,t)=t^3y^2$
  schreiben und somit ist $t^3y^2=C$ einen allgemeine Lösung der
  Differentialgleichung. Die Funktion $\Xi(y,t)$ heisst integrierender Faktor.
\end{note}
Angenommen (\ref{eq:DGLVarKoeff}) sei keine exakte Differentialgleichung, 
dann müssen wir einen solchen versuchen zu finden
\begin{enumerate}
    \item Wenn 
    \[\frac{\frac{\partial p(t,y)}{\partial y}-\frac{\partial q(t,y)}{\partial t}}{q(t,y)}=f(t)\] 
    eine Funktion von t allein ist, dann ist $e^{\int f(t)dt}$ ein integrierender Faktor. 
    Ebenso wenn
    \[\frac{\frac{\partial p(t,y)}{\partial y}-\frac{\partial q(t,y)}{\partial t}}{p(t,y)}=-g(y)\]
    nur eine Funktion von y ist, dann ist $e^{\int g(y)dy}$ ein integrierender Faktor.
    \item Ist (\ref{eq:DGLVarKoeff}) homogen, d.h. sind $p(t,y)$ und $q(t,y)$ homogene Funktionen vom selben Grad, und $p(t,y)\cdot t+q(t,y)\cdot y\ne 0$, dann ist
    $\frac{1}{p(t,y)\cdot t+q(t,y)\cdot y}$ ein integrierender Faktor.
    \item Wenn (\ref{eq:DGLVarKoeff}) in der Form $y\cdot f(t\cdot y)dt+t\cdot g(t\cdot y)dy=0$
    geschrieben werden kann, dann ist $\frac{1}{t\cdot y(f(t\cdot y)-g(t\cdot y))}=\frac{1}{p\cdot t-q\cdot y}$ ein integrierender Faktor.
    \item Durch genaues Hinschauen kann man durch Umgruppieren von Termen einen
      integrierenden Faktor finden, wenn man bestimmte Gruppen von Termen als
      Teil eines vollständigen Differentials identifiziert.
\end{enumerate}
%\begin{example}{}
%Gruppe von Termen, Integrierender Faktor, Vollständiges Differential.
%\end{example} \newpage
%
\section{Gewöhnliche lineare Differentialgleichungen}
\subsection{Gleichungen mit konstanten Koeffizienten}
Eine lineare Differentialgleichung n-ter Ordnung ist ein Zusammenhang folgender
Art
\begin{equation}\label{eq:DGLOrdnungN}
a_n\frac{d^n y(t)}{dt^n}+a_{n-1}\frac{d^{n-1} y(t)}{dt^{n-1}}+
\cdots +a_{1}\frac{d y(t)}{dt}+a_0y(t)=f(t)
\end{equation}
Dabei ist $y(t)$ eine Funktion der unabhängigen Veränderlichen $t$.
Differentialgleichungen vom Typ (\ref{eq:DGLOrdnungN}) beschreiben zahlreiche
Phänomene in Naturwissenschaft und Technik. Zunächst einmal wollen wir die
linearen Differentialgleichungen mit konstanten Koeffizienten untersuchen. Die
Funktion $f(t)$, Inhomogenität genannt, sei nur von der unabhängigen
Veränderlichen $t$ ab\-hängig. Wenn $f(t)=0$ nennen wir (\ref{eq:DGLOrdnungN})
eine homogene Differentialgleichung, wenn dem nicht so ist, heisst
(\ref{eq:DGLOrdnungN}) inhomogen.

Man beachte, dass die Koeffizienten der Ableitungen von $y(t)$ nicht
notwendigerweise Konstanten sein müssen, sondern bekannte Funktionen $a_k(t)$
sein können. Wichtig für die Bezeichnung als lineare Differentialgleichung ist
ganz einfach die Tatsache, dass das Superpositionsprinzip gilt. 
\begin{note}{Lineare Superposition von Lösungen}
D.h. wenn $y_a(t)$ und $y_b(t)$ je eine Lösung der homogenen linearen
Differentialgleichung sind, dann ist auch $y(t)=\alpha y_a(t) + \beta y_b(t)$
eine Lösung. Es löse $y_f(t)$ die Differentialgleichung mit der Inhomogenität
$f(t)$ und $y_g(t)$ diejenige mit der Inhomogenität $g(t)$, dann löst
$y(t)=\alpha y_f(t) + \beta y_g(t)$ die Differentialgleichung mit der
Inhomogenität $h(t)=\alpha f(t) + \beta g(t)$. Man beweise diese Aussage!
\end{note}
%
\subsection{Lösung der Gleichungen mit konstanten Koeffizienten}
Zur Lösung der Gleichungen vom Typ (\ref{eq:DGLOrdnungN}) haben wir
verschiedene Herangeshensweisen. Dabei gehen wir davon aus, dass es genau eine
Lösung von (\ref{eq:DGLOrdnungN}) mit $f(t)=0$, also der homogenen
Differentialgleichung, gibt, die den Bedingungen 
\begin{equation}
y(t_0)=b_0,\,\dot{y}(t_0)=b_1,\,\dots y^{(n-1)}(t_0)=b_{n-1}
  \label{eq:ICsGDGL}
\end{equation}
genügt. Man möge den Beweis dieses Satzes in den einschlägigen Lehrbüchern
nachlesen.
\subsubsection{Taylorreihe und  Potentzreihenansatz}
Zur Lösung von (\ref{eq:DGLOrdnungN}) liegt es nahe, den Ansatz
\begin{equation}
  y(t)=\sum\limits_{\nu=0}^{\infty}c_\nu t^\nu
  \label{eq:AnsatzPotenz}
\end{equation}
zu versuchen. D.h. wir vermuten die Lösung in der Form (\ref{eq:AnsatzPotenz})
finden zu können. Hierzu gibt es zwei Herangehensweisen. Einmal mit Hilfe der
Taylorreihe und zum anderen durch direktes Einsetzen von
(\ref{eq:AnsatzPotenz}). 

Wir arbeiten beides am Beispiel des Relaxators aus.
\begin{example}{Taylorreihe verglichen mit Potenzreihenansatz}
  Gegeben die homogene Differentialgleichung $\dot{y}(t)+a_0y(t)=0$ mit
  Anfangsbedingung $y(0)=b_0$ - wir setzen o.B.d.A. $t_0=0$. Wir haben
  \[ \dot{y}(t)=-a_0y(t),\, \ddot{y}(t)=-a_0\dot{y}(t),\,\dots\, 
      y^{(n)}(t)=-a_0y^{(n-1)}(t)\]
  Damit erhalten wir die Ableitungen bei $t=0$
  \[ y(0)=b_0,\, \dot{y}(0)=-a_0b_0,\, \ddot{y}(0)=a_0^2b_0,\,\dots\, 
     y^{(n)}(0)=(-1)^na_0^nb_0\]
  und können die Taylorreihe für $y(t)$ um $t=0$ schreiben als
  \begin{equation}
    y(t)=b_0\sum\limits_{\nu=0}^{\infty}(-1)^\nu\frac{a_0^\nu}{\nu!}t^\nu=b_0e^{-a_0t}
    \label{eq:SoluTaylor}
  \end{equation}
Wenn wir (\ref{eq:AnsatzPotenz}) direkt einsetzen, dann erhalten wir die Lösung
durch Koeffizientenvergleich.
  \[ \sum\limits_{\nu=0}^{\infty}\nu c_\nu t^{\nu-1}+ 
      a_0\sum\limits_{\nu=0}^{\infty}c_\nu t^\nu=
      \sum\limits_{\mu=0}^{\infty}(\mu+1)c_{\mu+1}t^\mu+ 
      a_0\sum\limits_{\nu=0}^{\infty}c_\nu t^\nu=
      \sum\limits_{\nu=0}^{\infty} \left((\nu+1)c_{\nu+1}+a_0c_\nu\right) t^\nu
  \]
  Aus der Anfangsbedingung erhalten wir eine Rekursionsformel für die $c_\nu$
  \begin{equation}
    (\nu+1)c_{\nu+1}+a_0c_\nu=0
    \label{eq:Rekursion}
  \end{equation}
  Mit $y(0)=b0$ haben wir $c_0=b_0$ und finden damit die $c_\nu$ aus
  (\ref{eq:Rekursion}) 
    \begin{equation}
      c_{\nu}=(-1)^\nu\frac{a_0^\nu b_0}{\nu!}\qquad (\nu=0,1,2,\dots).
    \label{eq:PotenzKoeffizienten}
  \end{equation}
  Mit den Koeffizienten (\ref{eq:PotenzKoeffizienten}) erhalten wir die Lösung
  \begin{equation}
    y(t)=\sum\limits_{\nu=0}^{\infty}(-1)^\nu\frac{a_0^\nu b_0}{\nu!}t^\nu=b_0e^{-a_0t}
    \label{eq:SoluPotenz}
  \end{equation}
\end{example}
%
\subsubsection{Der Lösungsansatz $y(t)=e^{\lambda t}$}
Setzen wir $y(t)=e^{\lambda t}$ in (\ref{eq:DGLOrdnungN}) ein, so erhalten wir
\begin{equation}
  \lambda^n+a_{n-1}\lambda^{n-1}+\dots +a_0=0
  \label{eq:Charakteristische}
\end{equation}
Die Gleichung (\ref{eq:Charakteristische}) hat $n$ Nullstellen und somit haben
wir auch $n$ Lösungen, die wir als Superposition zur Lösung des
Anfangswertproblems angeben können. Für unseren Relaxator bedeutet dies
\[ \lambda e^{\lambda t}+a_0e^{\lambda t}=0\]
Und daraus erhalten wir die Lösung $y(t)=ce^{-a_0 t}$. Um die Anfangsbedingung
zu erfüllen, setzen wir $c=b_0$.

\begin{example}{Der Exponentialansatz für eine Gleichung 2. Ordnung}
  Gegeben die Gleichung $\ddot{y}(t)+2\dot{y}(t)-3y(t)=0$ mit den
  Anfangsbedingungen $y(0)=2$ und $\dot{y}(0)=-2$. Die charakteristische
  Gleichung (\ref{eq:Charakteristische}) lautet für diesen Fall
  $\lambda^2+2\lambda-3=0$ mit den beiden Lösungen $\lambda_1=1$ und
  $\lambda_2=-3$. Wir schreiben die Lösung der Gleichung als Superposition
  $y(t)=c_1e^{\lambda_1 t}+c_2e^{\lambda_2 t}=c_1e^{t}+c_2e^{-3 t}$. Wenn wir
  die beiden Anfangsbedingunen erfüllen, dann sind $c_1=c_2=1$ festgelegt.
  Damit ist die Lösung $y(t)=e^t+e^{-3t}$.
\end{example}
Damit erhebt sich die berechtigte Frage, warum man den Potenzreihenansatz dann
überhaupt in Betracht zieht. Wir erörten dies an folgendem Beispiel:
\begin{example}{Mehrfachlösungen des charakteristishen Polynoms}
  \[\ddot{y}(t)-2\dot{y}(t)+y(t)=0,\mbox{ mit } y(0)=1,\quad \dot{y}(0)=2\]
Mit dem Exponentialansatz $e^{\lambda t}$ erhalten wir das charakteristische
Polynom $\lambda^2-2\lambda+1=(\lambda -1)^2=0$. Dieses hat eine
doppelte Nullstelle, aber wie lautet die zweite Lösung?
\end{example}
Versuchen wir es mit dem Potenzreihenansatz (\ref{eq:AnsatzPotenz}) wir
erhalten die Rekursionsformel
\[ (\nu+1)(\nu+2)c_{\nu+2}-2(\nu+1)c_{\nu+1}+c_\nu=0\]
Mit $d_\nu=c_{\nu}\nu!$ erhalten wir die Rekursionsformel
$d_{\nu+2}-2d_{\nu+1}+d_\nu=0$, wobei wegen der Anfangsbedingungen $d_0=c_0=1$
und $d_1=c_1=2$ sein muss und wir sehen dass $d_\nu=\nu+1$. Damit ist aber
\[ 
  y(t)=\sum\limits_{\nu=0}^{\infty}\frac{1+\nu}{\nu!}t^\nu=
        \sum\limits_{\nu=0}^{\infty}\frac{t^\nu}{\nu!}
       +\sum\limits_{\nu=1}^{\infty}\frac{t^\nu}{(\nu-1)!}=
        e^t+t\sum\limits_{\mu=0}^{\infty}\frac{t^\mu}{\mu!}=e^t+te^t
\]       
Dies legt die Vermutung nahe, dass bei einer r-fachen Lösung $\lambda_1$ des
charakteristischen Polynoms neben $e^{\lambda_1 t}$ auch $te^{\lambda_1
t},\dots,t^{r-1}e^{\lambda_1 t}$ Lösungen sind. Man zeige dies durch einsetzen
der einzelnen Lösungen in die homogene Differentialgleichung
$(D-\lambda_1)^r[y(t)]=0$!
%
\subsubsection{Das Fundamentalsystem von Lösungen}
Die Frage nach der Zahl der Lösungen soll im folgenden behandelt werden, d.h.
wir suchen das Fundamentalsystem von Lösungen zu (\ref{eq:DGLOrdnungN}).
\begin{example}{Fundamentalsystem der Gleichung 2. Ordnung}
  Wir wissen, dass für die Gleichung 2. Ordnung 
  \[\ddot{y}(t)+a_0y(t)=0 \]
  zwei Bedingungen vorliegen müssen, damit die Lösung eindeutig angegebn werden
  kann. Also nehmen wir die Bedingunen
  \begin{align}
  y(0)=&b_0\mbox{ und }\dot{y}(0)=0\mbox{ oder}\nonumber\\
  y(0)=&0\mbox{ und }\dot{y}(0)=b_1\nonumber\\
  \end{align}
  Der Potenzreihenansatz
  \[ y(t)=\sum\limits_{\nu=0}^\infty c_\nu t^\nu \]
  führt auf
  \begin{align*}
  \sum\limits_{\nu=2}^\infty\nu(\nu-1)c_\nu t^{\nu-2}+a_0\sum\limits_{\nu=0}^\infty c_\nu t^\nu &=0\\
  \sum\limits_{\nu=0}^\infty\left( (\nu+2)(\nu+1)c_{\nu+2} +a_0c_\nu\right) t^\nu &=0
  \end{align*}
  und mit der ersten Anfangsbedingung auf die Lösungen $y_1(t)$ mit
  \[ c_{2\mu}=(-1)^\mu\frac{a_0^\mu b_0}{(2\mu)!},\quad c_{2\mu+1}=0\]
  während die zweite Anfangsbedingung auf die Lösung $y_2$ mit
  \[ c_{2\mu}=0, \quad c_{2\mu+1}=(-1)^\mu\frac{a_0^\mu b_1}{(2\mu+1)!}\]
  führt. Damit erfüllt die Summe der beiden Lösungen die allgemeine
  Anfangsbedingung
  \[ y(0)=b_0\mbox{ und }\dot{y}(0)=b_1\]
\end{example}
Das obige Beispiel legt nahe, bei homogenen Differentiagleichungen n-ter
Ordnung, wie in (\ref{eq:DGLOrdnungN}) mit $f(t)=0$ angegeben, ähnlich
vorzugehen, indem wir der Reihe nach folgende Anfangsbedingungen aufstellen
\begin{align*}
  y(t_0)=1,\dot{y}(t_0)=0,&\dots\dots\dots,y^{(n-1)}(t_0)=0;\\
  y(t_0)=0,\dot{y}(t_0)=1,&\dots\dots\dots,y^{(n-1)}(t_0)=0;\\
  &\dots\dots\dots \\
  y(t_0)=0,\dot{y}(t_0)=0,&\dots\dots\dots,y^{(n-1)}(t_0)=1;
  \label{eq:NICs}
\end{align*}
Wir bezeichnen die entsprechenden Lösungen mit $y_1(t),y_2(t),\dots,y_n(t)$.
Für die Anfangsbedingung
$y(t_0)=b_0,\dot{y}(t_0)=b_1,\dots,y^{(n-1)}(t_0)=b_{n-1}$ können wir damit
sofort die Lösung in der Form
\begin{equation}
  y(t)=b_0y_1(t)+b_1y_2(t)+\dots+b_{n-1}y_n(t)
  \label{eq:FundamentalSuperpose}
\end{equation}
angeben. Dies ist allerdings bisher auf die Stelle $t_0$ beschränkt, wir wollen
versuchen eine allgemeine Aussage zu machen. Wir bilden mit den obigen Lösungen 
\[y(t)=c_1y_1(t)+c_2y_2(t)+\dots+c_ny_n(t)\qquad (c_1,c_2,\dots,c_n\in\mathbb{R})\]
was wieder Lösung der Differentialgleichung (\ref{eq:DGLOrdnungN}) ist.

Das lineare Gleichungssystem
\begin{align}
c_1y_1(t_0)+c_2y_2(t_0)&+{\dots\dots\dots}+c_ny_n(t_0)=b_0\nonumber\\
c_1\dot{y}_1(t_0)+c_2\dot{y}_2(t_0)&+{\dots\dots\dots}+c_n\dot{y}_n(t_0)=b_1\nonumber\\
c_1\ddot{y}_1(t_0)+c_2\ddot{y}_2(t_0)&+{\dots\dots\dots}+c_n\ddot{y}_n(t_0)=b_2\nonumber\\
  &\dots\dots\dots \\
  c_1y^{(n-2)}_1(t_0)+c_2y^{(n-2)}_2(t_0)&+{\dots\dots\dots}+c_ny^{(n-2)}_n(t_0)=b_{n-2}\nonumber\\
  c_1y^{(n-1)}_1(t_0)+c_2y^{(n-1)}_2(t_0)&+{\dots\dots\dots}+c_ny^{(n-1)}_n(t_0)=b_{n-1}\nonumber
  \label{eq:LGSatt0}
\end{align}
hat die eindeutige Lösung $c_1=b_0$, $c_2=b_1$,\dots, $c_n=b_{n-1}$. Das
bedeutet dass die Koeffizientendeterminante
\begin{equation}
  W(t_0)=\begin{vmatrix}
           y_1(t_0)&y_2(t_0)&\quad\dots\quad &y_n(t_0)\\
           \dot{y}_1(t_0)&\dot{y}_2(t_0)&\quad\dots\quad &\dot{y}_n(t_0)\\
           \ddot{y}_1(t_0)&\ddot{y}_2(t_0)&\quad\ddots\quad &\ddot{y}_n(t_0)\\
	   \dots&\dots &\quad\dots\quad&\dots\\
           y^{(n-2)}_1(t_0)&y^{(n-2)}_2(t_0)&\quad\dots\quad &y^{(n-2)}_n(t_0)\\
           y^{(n-1)}_1(t_0)&y^{(n-1)}_2(t_0)&\quad\dots\quad &y^{(n-1)}_n(t_0)
         \end{vmatrix}
  \label{eq:Wronskiant0}
\end{equation}
von null verschieden ist. Die Determinante (\ref{eq:Wronskiant0}), wird
Wronskideterminante genannt und sie ist also, wenn eine Lösung für die
Anfangsbedingungen bei $t_0$ vorliegt, von null verschieden. Wenn dies aber für
beliebige Stellen $t$ der Fall sein soll, müssen wir das allgemeine Verhalten
der Wronskideterminante untersuchen, d.h. sie muss für jedes beliebige $t$ von
null verschieden sein.

Dazu bilden wir die Ableitung von $W(t)$ und erhalten
\begin{align}
  \dot{W}(t)=&\begin{vmatrix}
    \dot{y}_1(t)&\quad\dots\quad &\dot{y}_n(t)\\
           \dot{y}_1(t)&\quad\dots\quad &\dot{y}_n(t)\\
           \ddot{y}_1(t)&\quad\ddots\quad &\ddot{y}_n(t)\\
                       &\quad\dots\quad&\dots\\
           y^{(n-1)}_1(t)&\quad\dots\quad &y^{(n-1)}_n(t)
         \end{vmatrix}+
	 \begin{vmatrix}
           y_1(t)&\quad\dots\quad &y_n(t)\\
           \ddot{y}_1(t)&\quad\dots\quad &\ddot{y}_n(t)\\
           \ddot{y}_1(t)&\quad\dots\quad &\ddot{y}_n(t)\\
                       &\quad\dots\quad&\dots\\
           y^{(n-1)}_1(t)&\quad\dots\quad &y^{(n-1)}_n(t)
         \end{vmatrix}+\dots\\
      \dots+&\begin{vmatrix}
           y_1(t)&\quad\dots\quad &y_n(t)\\
           \dot{y}_1(t)&\quad\dots\quad &\dot{y}_n(t)\\
                       &\quad\dots\quad&\dots\\
           y^{(n-1)}_1(t)&\quad\dots\quad &y^{(n-1)}_n(t)\\
           y^{(n-1)}_1(t)&\quad\dots\quad &y^{(n-1)}_n(t)
         \end{vmatrix}+
      \begin{vmatrix}
           y_1(t)&\quad\dots\quad &y_n(t)\\
           \dot{y}_1(t)&\quad\dots\quad &\dot{y}_n(t)\\
                       &\quad\dots\quad&\dots\\
           y^{(n-2)}_1(t)&\quad\dots\quad &y^{(n-2)}_n(t)\\
           y^{(n)}_1(t)&\quad\dots\quad &y^{(n)}_n(t)
         \end{vmatrix}
  \label{eq:WronskianDerivative1}
\end{align}
Nur die letzte der resultierenden Determinanten ist von null verschieden.
O.b.d.A. nehmen wir $a_n=1$ in (\ref{eq:DGLOrdnungN}) an\footnote[1]{Wir teilen
(\ref{eq:DGLOrdnungN}) durch $a_n$, vorausgesetzt $a_n\ne0$, denn wenn das
nicht der Fall ist, liegt eine Differentialgleichung (n-1)-ter Ordnung vor.}
und erhalten damit
\begin{equation}
  \begin{vmatrix}
           y_1(t)&\quad\dots\quad &y_n(t)\\
           \dot{y}_1(t)&\quad\dots\quad &\dot{y}_n(t)\\
                       &\quad\dots\quad&\dots\\
      -a_{n-1}y^{(n-1)}_1(t)-\dots-a_0y_1&\quad\dots\quad 
      &-a_{n-1}y^{(n-1)}_n(t)-\dots-a_0y_n(t)
         \end{vmatrix}
  \label{eq:WronskianDerivative2}
\end{equation}
Nun multiplizieren wir die erste Zeile von (\ref{eq:WronskianDerivative2}) mit
$a_0$ und addieren sie zur letzten Zeile, sukzessive bis $a_{n-2}$
multipliziert mit den vorletzten Zeile und wir erhalten
\begin{equation}
  \dot{W}(t)=-a_{n-1}W(t)
  \label{eq:WronskianDGL}
\end{equation}
Für diese Differentialgleichung haben wir eine Anfangsbedinung $W(t_0)=w_0$ und
können sie damit lösen. Wir erhalten
\begin{equation}
  W(t)=w_0e^{-a_{n-1}(t-t_0)}
  \label{eq:WronskianGeneral}
\end{equation}
Da die Exponentialfunktion keine Nullstellen hat, schließen wir aus
(\ref{eq:WronskianGeneral}), dass wenn $W(t)$ an irgendeiner Stelle von null
verschieden ist, sie üeberall von null verschieden ist. Wir sehen also, da
$W(t)$ im gegebenen Fall nicht null werden kann, dass immer eine Lösung
existiert.
\begin{note}{}
  Beachte das über multiple Lösungen des charakteristischen Polynoms oben
  gesagte!
\end{note}
\subsection{Lösung der inhomogenen Gleichung: Operatormethode}
Für die Ableitung benutzen wir als Abkürung eine Operatorschreibweise der Form
$\frac{dy(t)}{dt}=D[y(t)]$. Entsprechend soll $D^{-1}[y(t)]=\int y(t)dt$
verwendet werden. Wir wissen ja aus dem Haupsatz der Differential- und
Integralrechnung, dass die Ableitung die Umkehrung des Integrals ist. Für einen
Operator $D$ schreiben wir dessen Inverse als $D^{-1}$. Ein r-faches
Hintereinanderausführen eines Operators können wir also mit $D^r$ schreiben.
\begin{note}{}
  Mit dieser Schreibweise wird uns klar, dass z.B. die Exponentialfunktion der
  Ableitung
  \[e^\frac{d}{dt}=e^D=1+D+\frac{D^2}{2}+\dots=\sum_{\nu=0}^\infty\frac{D^\nu}{\nu!}\]
  bedeutet. 
  
  Merke: Funktionen von Operatoren sind über ihre Reihenentwicklung
  definiert.

  Eine offensichtliche Anwendung sehen wir mit der Taylorentwicklung von
  $y(t+\tau)$ um den Zeitpunkt $t$
  \[y(t+\tau)=y(t)+\left.\frac{\tau}{1!}\dot{y}(s)\right|_{s=t}+\left.\frac{\tau^2}{2!}\ddot{y}(s)\right|_{s=t}
	+\dots=\sum\limits_{\nu=0}^{\infty}\left.\frac{\tau^\nu}{\nu!}\frac{d^\nu}{ds^\nu}y(s)\right|_{s=t}=
	e^{\tau\frac{d}{dt}}y(t)\]
  Wir nennen $e^{\tau\frac{d}{dt}}=e^{\tau D}$ den Verschiebungs- oder
  Shiftoperator.
\end{note}
Wir betrachten die folgende Differentialgleichung erster Ordnung mit konstanten
Koeffizienten 
\begin{equation}
  \dot{y}(t)-\lambda y(t)=g(t)\mbox{ oder in Operatorform }(D-\lambda)y(t)=g(t)
  \label{eq:RelaxatorOperatorform}
\end{equation}
Wir wissen, dass (\ref{eq:RelaxatorOperatorform}) als homogene Lösung
$y(t)=e^{\lambda t}$ hat, also versuchen wir es mit dem
Produktansatz\footnote[1]{Eingesetzt ergibt dies $D[e^{\lambda
t}D^{-1}[e^{-\lambda t}g(t)]]-\lambda e^{\lambda t}D^{-1}[e^{-\lambda
t}g(t)]=g(t)$. Wenn wir die Operation $D$ ausführen - Achtung Produktregel -
und $DD^{-1}=1$ beachten, dann erhalten wir $g(t)=g(t)$}
\begin{equation}
  y(t)=e^{\lambda t}\cdot D^{-1}[e^{-\lambda t}g(t)]
  \label{eq:Produktansatz}
\end{equation}
Wir verifizieren das durch einsetzen in (\ref{eq:RelaxatorOperatorform}). Nun
wenden wir das auf die Differentialgleichung $(D-\lambda)^2[y(t)]=g(t)$ an und
machen hierfür den entsprechenden Produktansatz
\begin{equation}
  y(t)=e^{\lambda t}\cdot D^{-2}[e^{-\lambda t}g(t)]
  \label{eq:Produktansatz2Ordnung}
\end{equation}
und zeigen wieder, dass (\ref{eq:Produktansatz2Ordnung}) die
Differentialgleichung erfüllt. nun stellt sich die Frage nach der
Verallgemeinerung, d.h. was geschieht wenn die Differentialgleichung 
\begin{equation}
  \ddot{y}(t)-\alpha\dot{y}(t)+\beta y(t)=g(t)
  \label{eq:DGL2Ordnung2Koeffizienten}
\end{equation}
vorliegt, woebei wir $\alpha=\lambda_1+\lambda_2$ und $\beta=\lambda_1\lambda_2$
schreiben. Damit haben wir
\begin{equation}
  (D-\lambda_1)(D-\lambda_2)[y(t)]=g(t)
  \label{eq:DGG2KoeffizientenOperatorform}
\end{equation}
Die Gleichung (\ref{eq:DGL2Ordnung2Koeffizienten}) können wir natürlich auch in
der Form $(D-\lambda_2)[y(t)]=y^*(t)$ und damit als
$(D-\lambda_1)[y^*(t)]=g(t)$ schreiben.

Nun lösen wir das ganze sukzessive auf
\begin{align} 
  y^*(t)&=e^{\lambda_1t}\cdot D^{-1}\left[e^{-\lambda_1 t}g(t)\right]\nonumber\\
  y(t)  &=e^{\lambda_2t}\cdot D^{-1}\left[e^{-\lambda_2 t}y^*(t)\right]=\nonumber\\
        &=e^{\lambda_2t}\cdot D^{-1}\left[e^{(\lambda_1-\lambda_2) t}\cdot
      D^{-1}\left[e^{-\lambda_1 t}g(t)\right]\right]
  \label{eq:DGL2OrdnungSolu}
\end{align}
Damit ist auch der allgemeine Fall für ein Polynom beliebiger Ordnung 
\[ P(D)[y(t)]=(D-\lambda_1)^{r_1}\cdot(D-\lambda_2)^{r_2}\cdots 
(D-\lambda_n)^{r_n}[y(t)]\]
zugänglich, nämlich
\begin{equation}
  y(t)=e^{\lambda_s t}\cdot D^{-r_s}[e^{(\lambda_{s-1}-\lambda_s)t}\cdot D^{-r_{s-1}}
  [\dots e^{(\lambda_1-\lambda_2)t}D^{-r_1}[e^{-\lambda_1t}g(t)]\dots]]
  \label{eq:SoluInhomogenOperator}
\end{equation}
\begin{example}{Ganzrationale Inhomogenität}
  \[\dddot{y}(t)-3\dot{y}(t)-2y(t)=4t^2-2\]
  Hat die characteristische Gleichung
  \[(\lambda^3-3\lambda-2)=(\lambda-2)(\lambda+1)^2=0\]
  Die allgemeine Lösung der homogenen Differentialgleichung lautet somit
  \[y(t)=c_1e^{2t}+c_2e^{-t}+c_3te^{-t}\]
  Mit (\ref{eq:SoluInhomogenOperator}) erhalten wir
  \[
    y(t)=e^{-t}\cdot D^{-2}[e^{3t}\cdot D^{-^1}[e^{-2t}(4t^2-2)]]=-2t^2+6t-8
  \]
  Damit ist die allgemeine Lösung gegeben durch
  \[ y(t)=c_1e^{2t}+c_2e^{-t}+c_3te^{-t}-2t^2+6t-8 \]
  Die $c_i$ werden durch die Anfangsbedingungen festgelegt.
\end{example}
Hieraus leiten wir eine allgemeine herangehensweise bei Polynomen als
Inhomogenitäten ab.
\begin{note}{Spezielle Lösung bei polynomialen Inhomogenitäten}
  Ist die Inhomogenität gegeben als ein Polynom n-ter Ordnung, so setzt man die
  spezielle Lösung der Differentialgleichung als allgemeines Polynom n-ter
  Ordnung an und bestimmt die Polynomialkoeffizienten durch einsetzen in die
  inhomogene Differetialgleichung und Koeffizientenvergleich.
\end{note}
Wir fassen die allgemeine Vorgehensweise im Diagramm in Abbildung
\ref{fig:FlowchartSolu} zusammen.
%
\begin{figure}[htb]
  \centering
  \tikzstyle{decision} = [diamond, draw, 
    text width=4.5em, text badly centered, node distance=3cm, inner sep=0pt]
  \tikzstyle{block} = [rectangle, draw, 
    text width=8em, text centered, rounded corners, minimum height=4em]
  \tikzstyle{line} = [draw, -latex']
  \tikzstyle{cloud} = [draw, ellipse, node distance=3cm,
    minimum height=2em]
\begin{tikzpicture}[node distance = 2cm, auto]
    % Place nodes
    \node [block] (init) {DGL-Löser};
    \node [block, left of=init, node distance=4cm] (dgl) {Differential- gleichung};
    \node [block, right of=init, node distance=4cm] (ics) {Anfangsbe- dingungen};
    \node [decision, below of=init] (hominhom) {ist es eine homogene DGL?};
    \node [block, right of=hominhom, node distance=4cm] (partsolu) {Suche spezielle L\"osung};
    \node [block, below of=hominhom, node distance=3cm] (homsolu) {Suche homogene L\"osung(en)};
    \node [block, below of=homsolu, node distance=2cm] (hometinhom) {Addiere
           homogene und inhomogene L\"osung(en)};
    \node [block, below of=hometinhom, node distance=2cm] (stop) {Erf\"ulle Randbedingungen};
         % Draw edges
    \path [line] (init) -- (hominhom);
    \path [line] (hominhom) -- node {immer} (homsolu);
    \path [line] (hominhom) -- node {yes} (partsolu);
    \path [line] (homsolu) -- (hometinhom);
    \path [line] (hometinhom) -- (stop);
    \path [line] (dgl) -- (init);
    \path [line] (ics) -- (init);
    \path [line] (partsolu) |- (stop);
\end{tikzpicture}
\caption{Flussdiagramm zur Lösung inhomogener linearer Differentialgleichungen.}
  \label{fig:FlowchartSolu}
\end{figure}
%
\begin{note}{Exponentielle Inhomogenität}
  Wir haben 2 Fälle zu betrachten
  \begin{enumerate}
    \item Ist $g(t)=e^{\mu t}$ und $\mu\ne\lambda_i$ dann erhalten wir durch
      einsetzen in (\ref{eq:SoluInhomogenOperator}) 
  \[ y(t)=\frac{e^{\mu t}}{(\mu-\lambda_1)^{r_1}
            \cdots(\mu-\lambda_s)^{r_s}}\]
	  \item Liegt hingegen Resonanz vor, z.B.  $\mu=\lambda_s$, dann schreiben wir (\ref{eq:SoluInhomogenOperator})
	    \[y(t)=e^{\lambda_s t}\cdot D^{-r_s}
	    \left[\frac{e^{\mu-\lambda_s}}{(\mu-\lambda_1)^{r_1}\dots(\mu-\lambda_{s-1})^{r_{s-1}}}\right]\]
	    Beachte, es die Reihenfolge der Operatoren $(D-\lambda_i)^{r_i}$
	    kann beliebig vertauscht werden und wir erhalten im Resonanzfall
	    \[y(t)=e^{\mu t}\cdot D^{-r_s}
	      \left[\frac{1}{(\mu-\lambda_1)^{r_1}\dots(\mu-\lambda_{s-1})^{r_{s-1}}}\right]\]
	    Was sich wiederum integrieren lässt zu
	\[ y(t)=\frac{t^{r_s}e^{\mu t}}{(\mu-\lambda_1)^{r_1} \cdots(\mu-\lambda_{s-1})^{r_{s-1}}}
	\cdot\frac{1}{r_s!}\]
  \end{enumerate}
\end{note}
%
\subsection{Lösung der inhomogenen Gleichung: Variation der Konstanten}
Diese Methode lässt sich auch bei variablen Koeffizienten anwenden. Die Lösung
der homogenen Gleichung hierfür, soll allerdings erst später besprochen werden.
Daher gehen wir von einer Differentialgleichung der Form (\ref{eq:DGLOrdnungN})
aus. Die Idee hinter der Methode ist die gefundenen Fundamentallösungen zu
superponieren, die benutzten Koeffizienten aber als noch zu bestimmende
Funktionen anzusetzen. Wir setzen also folgendes an
\begin{equation}
  y(t)=c_1(t)y_1(t)+c_2(t)y_2(t)+\cdots+c_n(t)y_n(t)
  \label{eq:AnsatzVariationKonstanten}
\end{equation}
Nun brauchen wir natürlich auch $n$ Bedingungen um die Funktionen $c_i(t)$ zu
bestimmen. Wir nehmen an dass die $c_i(t)$ mindestens einmal differenzierbar seien.
Die Lösung $y(t)$ hingegen muss n-mal differenzierbar sein. Dann legen wir folgendes fest für die $c_i(t)$
\begin{align}
  \dot{y}(t)=&c_1(t)\dot{y}_1(t)+c_2(t)\dot{y}_2(t)+\cdots+c_n(t)\dot{y}_n(t)\nonumber\\
  &+\underbrace{\dot{c}_1(t)y_1(t)+\dot{c}_2(t)y_2(t)+\cdots+\dot{c}_n(t)y_n(t)}_{=0}\nonumber\\
  \ddot{y}(t)=&c_1(t)\ddot{y}_1(t)+c_2(t)\ddot{y}_2(t)+\cdots+c_n(t)\ddot{y}_n(t)\nonumber\\
  &+\underbrace{\dot{c}_1(t)\dot{y}_1(t)+\dot{c}_2(t)\dot{y}_2(t)+\cdots
               +\dot{c}_n(t)\dot{y}_n(t)}_{=0}\nonumber\\
	       &\cdots\cdots\cdots\nonumber\\
	       y^{(n)}(t)=&c_1(t)y^{(n)}_1(t)+c_2(t)y^{(n)}_2(t)+\cdots+c_n(t)y^{(n)}_n(t)\nonumber\\
  &+\underbrace{\dot{c}_1(t)y^{(n-1)}_1(t)+\dot{c}_2(t)y^{(n-1)}_2(t)+\cdots
  +\dot{c}_n(t)y^{(n-1)}_n(t)}_{=g(t)}\label{eq:Diffbarkeitci}
\end{align}
Gegeben, dass diese Bedingungen erfüllt sind, so ist
(\ref{eq:AnsatzVariationKonstanten}) Lösung der Differentialgleichung. Dies
können wir sofort erkennen, denn wenn wir die so berechneten Ableitungen aus
(\ref{eq:Diffbarkeitci}) in die Differentialgleichung einsetzen, ist diese
erfüllt.

Für die $\dot{c}_k(t)$ dagegen habe wir das Gleichungssystem
\begin{align}
  \dot{c}_1(t)y_1(t)&+\dot{c}_2(t)y_2(t)+\cdots+\dot{c}_n(t)y_n(t)=0\nonumber\\
	       &\cdots\cdots\cdots\nonumber\\
  \dot{c}_1(t)y^{(n-1)}_1(t)&+\dot{c}_2(t)y^{(n-1)}_2(t)+\cdots
  +\dot{c}_n(t)y^{(n-1)}_n(t)=g(t)
	       \label{eq:SystemCs}
\end{align}
Die Koeffizientendeterminante von (\ref{eq:SystemCs}) ist die
Wronskideterminante von der wir in (\ref{eq:WronskianGeneral}) gezeigt haben,
dass sie von null verschieden ist. Damit ist sichergestellt, dass
(\ref{eq:SystemCs}) eine Lösung hat.
%
\begin{example}{Variation der Konstanten}
  \[\ddot{y}(t)+y(t)=\frac{1}{\cos(t)}\]
  %TODO die homogene Lösungen sind ?
  Die allgemeine Lösung ist $y(t)=c_1\cos(t)+c_2\sin(t)$ und wir machen für die
  spezielle Lösung den Ansatz
  \[y(t)=c_1(t)\cos(t)+c_2(t)\sin(t)\]
  und erhalten das lineare Gleichungssystem
  \begin{align*}
    \dot{c}_1(t)\cos(t)+\dot{c}_2(t)\sin(t)&=0\\
    -\dot{c}_1(t)\sin(t)+\dot{c}_2(t)\cos(t)&=\frac{1}{\cos(t)}
  \end{align*}
  Die spezielle Lösung ist damit
  \[y(t)=\cos(t)\ln(|\cos(t)|)+t\sin(t)\]
\end{example}
%
\pagebreak 
%
\subsection{Systeme linearer gewöhnlicher Differentialgleichungen}
Wir benutzen die Operatorschreibweise, um ein allgemeines System von
Differentialgleichungen beliebiger Ordnung anzugeben. Wir betrachten somit
Systeme der Art
\begin{equation}
  P_{i1}(D)[y_1(t)]+P_{i2}(D)[y_2(t)]+\dots+P_{in}(D)[y_n(t)]=f_i(t)\hfill
  (i=1,2,\dots,m)
  \label{eq:SystemGeneral}
\end{equation}
Um (\ref{eq:SystemCs}) zu lösen gibt es zwei Strategien: Erstens könnten wir
aus dem vorliegenden System eine Differentialgleichung höherer Ordnung machen
oder zweitens könnten wir versuchen auf ein System von gekoppelten
Differentialgleichungen erster Ordnung überzugehen.

Anhand zweier Beispiele wollen wir die Vor- und Nachteile beider Methoden
untersuchen und aus den Nachteilen gegebenenfalls Rückschlüsse auf die Modelle
ziehen.
\begin{example}{Rückführung auf DGL höherer Ordnung}
  Ähnlich dem Gauß'schen Eliminationsverfahren schreiben wir
  \begin{equation*}
    \begin{array}{rrl|l}
      D^2[y_1(t)]+&y_2(t)&=0&D^2\\
      y_1(t)+&D^2[y_2(t)]&=0&\cdot(-1)
    \end{array}
  \end{equation*}
  und erhalten die DGL $D^4[y_1(t)]-y_1(t)=0$.

Versuchen wir die Methode hingegen auf das System
\begin{align*}
  (D+1)[y_1(t)]+(D^2-1)[y_2(t)]&=0\\
  y_1(t)+(D-1)[y_2(t)]&=0
\end{align*}
anzuwenden, gelingt die Elimination von $y_1(t)$ oder $y_2(t)$ nicht, denn die
beiden Gleichungen sind linear abhängig (warum?).
\end{example}
Wie wir aus dem Gegenbeispiel gesehen haben, ist die Tatsache der linearen
Abhängigkeit auf eine nicht eindeutige Formulierung der Problemstellung
zurückzuführen. Denn entweder ist $y_1(t)$ oder $y_2(t)$ frei wählbar und die
Gleichung dann nach der jeweils anderen Funktion zu lösen. Diese Wahlfreiheit
bedeutet aber einen Mangel an Information über das Modellsystem. Für eine
eindeutige Lösung müssen wir uns Gedanken über weitere Bedingungen machen, die
wir dem System auferlegen, damit es eindeutig mit Hilfe der
Differentialgleichungen formuliert werden kann. Das beudeutet dann noch nicht,
dass es lösbar sein wird.
\begin{example}{Rückführung auf ein System linearer DGL 1. Ordnung}
  Aus dem System
  \begin{align*}
    \ddot{y}_1(t)+y_2(t)=0\\
    \ddot{y}_2(t)+y_1(t)=0
  \end{align*}
  erhalten wir mit $\dot{y}_1(t)=y_3(t)$ und $\dot{y}_2(t)=y_4(t)$ das System
  \begin{align*}
    \dot{y}_1(t)=y_3(t)\\
    \dot{y}_2(t)=y_4(t)\\
    \dot{y}_3(t)=-y_2(t)\\
    \dot{y}_4(t)=-y_1(t)
  \end{align*}
  Versuchen wir allerdings diese Methode auf das System
    \begin{align*}
      \ddot{y}_2(t)+\dot{y}_1(t)+y_1(t)-y_2(t)=0\\
      \dot{y}_2(t)+y_1(t)-y_2(t)=0
  \end{align*}
  anzuwenden, dann lässt sich dieses nicht in ein System der gewünschten Art
  überführen. In diesem Fall lässt sich die Funktion $y_1(t)$ frei wählen und
  dann das lineare System lösen. Da aber von vornherein $y_1(t)$ nicht
  eindeutig bestimmt ist, gilt das oben gesagte.
\end{example}
Wir werden uns im folgenden daher nur mit der Form
\[
  \dot{y}_i(t)=a_{i1}y_1(t)+a_{i2}y_2(t)+\dots+a_{in}y_n(t)\hfill (i=1,2,\dots,m)
\]
befassen.
\subsection{Homogene Systeme erster Ordnung}
Ein homogenes System von linearen Differentialgleichungen erster Ordnung hat
die Form
\begin{equation}
  \begin{pmatrix}
    \dot{y}_1(t)\\
    \dot{y}_2(t)\\
    \dots\\
    \dot{y}_n(t)
  \end{pmatrix}
  =
  \begin{pmatrix}
    a_{11}&a_{12}&\dots &a_{1n}\\
    a_{21}&a_{22}&\dots &a_{2n}\\
    \multicolumn{4}{c}\dotfill\\
    a_{n1}&a_{n2}&\dots &a_{nn}\\
  \end{pmatrix}
  \begin{pmatrix}
    y_1(t)\\
    y_2(t)\\
    \dots\\
    y_n(t)
  \end{pmatrix}
  \label{eq:System1stOrder}
\end{equation}
Das System (\ref{eq:System1stOrder}) lässt sich mit Hilfe der
Operatorschreibweise auch so darstellen
\begin{equation}
  \begin{pmatrix}
    a_{11}-D&a_{12}&\dots &a_{1n}\\
    a_{21}&a_{22}-D&\dots &a_{2n}\\
    \multicolumn{4}{c}\dotfill\\
    a_{n1}&a_{n2}&\dots &a_{nn}-D\\
  \end{pmatrix}
  \begin{pmatrix}
    y_1(t)\\
    y_2(t)\\
    \dots\\
    y_n(t)
  \end{pmatrix}
  =0
  \label{eq:System1stOperator}
\end{equation}
Oder wir schreiben es ganz einfach als
\begin{equation}
  \dot{\vec{y}}(t)=A\cdot\vec{y}(t)
  \label{eq:SystemVectorMatrixNotation}
\end{equation}
Es gibt genau eine Lösung $(y_1(t),y_2(t),\dots,y_n(t))$ von
(\ref{eq:System1stOrder}), so dass die zu einem beliebigen Zeitpunkt $t_0$
gewählten Bedingungen
\begin{equation}
  y_1(t_0)=b1,\:y_2(t_0)=b_2,\dots,y_n(t_0)=b_n
  \label{eq:ICsSystem}
\end{equation}
erfüllt sind. Wir werden den Beweis hier nicht antreten und verweisen
stattdessen auf die einschlägigen Lehrbücher.

Für die Lösung der Systeme (\ref{eq:System1stOrder}) benutzen wir den
Exponentialansatz $e^{\lambda t}$. Da wir $n$ Funktionen zu bestimmen haben,
schreiben wir diesen gleich in der Form
\begin{equation}
  y_i(t)=c_ie^{\lambda t}\hfill (i=1,2,\dots,n\mbox{ und }c_i\in\mathbb{R})
  \label{eq:SystemAnsatz}
\end{equation}
Wir setzen den Ansatz in (\ref{eq:System1stOrder}) ein und erhalten das
Gleichungssystem
\begin{align}
    (a_{11}-\lambda)c_1+a_{12}c_2+\dots+a_{1n}c_n&=0\nonumber\\
    a_{21}c_1+(a_{22}-\lambda)c_2+\dots+a_{2n}c_n&=0\nonumber\\
    \dots\dots\dots\dots\nonumber\\
    a_{21}c_1+a_{22}c_2+\dots+(a_{2n}-\lambda)c_n&=0
  \label{eq:System4cs}
\end{align}
Oder einfacher
\begin{equation}
  (A-\lambda\mathbbm{1})\cdot\vec{c}=0
  \label{eq:Vektorform}
\end{equation}
Aus der linearen Algebra wissen wir, dass (\ref{eq:Vektorform}) nur eine
Lösung hat, wenn gilt $|A-\lambda\mathbbm{1}|=0$ und die für die Lösungen
gewählten Werte der $\lambda$ sind die Eigenwerte der Koeffizientenmatrix $A$.

Um (\ref{eq:Vektorform}) zu lösen erinnern wir uns an Eigenwerte, Eigenvektoren
und Normalformen quadratischer Matrizen. Wir wissen es gibt drei Fälle:
\begin{enumerate}
  \item Alle Eigenwerte $\lambda_i$ sind voneinander verschieden.
  \item Es treten mehrfache Eigenwerte auf. Der Rang der Matrix
    $A-\lambda\mathbbm{1}$ fällt für jeden mehrfachen Eigenwert um soviel
    gegenüber $n$, wie die Vielfachheit des entsprechenden Eigenwertes beträgt.
  \item Es tritt mindestens ein mehrfacher Eigenwert auf für den der Rangabfall
    nicht die obere Regel erfüllt.
\end{enumerate}
Die ersten beiden Fälle erlauben uns die Matrix $A$ auf Diagonalform 
\begin{equation}
  \Lambda=
  \begin{pmatrix}
    \lambda_1&0&\dots&0\\
    0&\lambda_2&\dots&0\\
    \multicolumn{4}{c}\dotfill\\
    0&0&\dots&\lambda_n
  \end{pmatrix}
  \label{eq:Diagonalform}
\end{equation}
zu bringen. Was bedeutet das für unser Gleichungssystem? Wir schreiben
$\vec{y}^T$ als Transformierte eines Vektors $\vec{z}^T$ dergestalt
$\vec{y}^T=C\cdot\vec{z}^T$ und damit auch $\dot{\vec{y}}^T = C\cdot
\dot{\vec{z}}^T$. Damit erhalten wir 
\[ C\cdot \dot{\vec{z}}^T = A\cdot C\cdot\vec{z}^T \]
und wenn wir mit $C^{-1}$ multiplizieren ergibt sich
\begin{equation}
  \dot{\vec{z}}^T = C^{-1}A\cdot C\cdot\vec{z}^T
  \label{eq:Diagonalsystem}
\end{equation}
Der Vorteil erschließt sich uns sofort: wir lösen das einfache Diagonalsysteme
(\ref{eq:Diagonalsystem}) und transformieren dann zurück um $\vec{y}(t)$ zu
erhalten.
%

\textbf{Fall 1: }Wir bestimmen zunächst die Eigenvektoren $(c_{i1}, c_{i2},
\dots, c_{in})$, die zu den jeweiligen Eigenwerten $\lambda_i$ gehören. Diese
bilden die Spalten der Transformationsmatrix 
\begin{equation}
  C=\begin{pmatrix}
    c_{11}&c_{21}&\dots&c_{n1}\\
    c_{12}&c_{22}&\dots&c_{n2}\\
    &&\dots&\\
    c_{1n}&c_{2n}&\dots&c_{nn}
  \end{pmatrix}
  \label{eq:Tranformationsmatrix}
\end{equation}
mit deren Hilfe wir die Matrix $A$ in die Diagonalform $\Lambda$ übeführen können
\begin{equation}
  C^{-1}\cdot A\cdot C=\Lambda
%  \label{eq:Diagonalform}
\end{equation}
Mit (\ref{eq:Tranformationsmatrix}) können wir die allgemeine Lösung angeben
\begin{equation}
  \begin{pmatrix}y_1(t)\\y_2(t)\\\dots\\y_n(t)\end{pmatrix}=
  \begin{pmatrix}
    c_{11}d_1&c_{21}d_2&\dots&c_{n1}d_n\\
    c_{12}d_1&c_{22}d_2&\dots&c_{n2}d_n\\
    &&\dots&\\
    c_{1n}d_1&c_{2n}d_2&\dots&c_{nn}d_n\\
  \end{pmatrix}\cdot
  \begin{pmatrix}e^{\lambda_1t}\\e^{\lambda_2t}\\\dots\\e^{\lambda_nt}\end{pmatrix}
  \label{eq:LsgAllgFall1}
\end{equation}
Die beliebig wählbaren reellen Konstanten $d_i$ benutzen wir zur Erfüllung der
Anfangsbedingungen.

\textbf{Fall 2: }Es treten mehrfache Eigenwerte auf und der Rangabfall der
Koeffizientenmatrix des Gleichungssystems aus (\ref{eq:System4cs}) ist gleich
dem Vielfachen des Eigenwertes. Auch in diesem Fall lässt sich $A$ in die
Diagonalform bringen. Wir erhalten parametrische Lösungen für die
Eigenvektroren der mehrfachen Eigenwerte, die wir so wählen können, dass sie
linear unabhängig sind. Das weitere Vorgehen ist genauso, wie in Fall 1.

\textbf{Fall 3: }In diesem Fall können wir die Matrix $A$ nicht auf
Diagonalform jedoch auf die Jordansche Normalform bringen.
\begin{equation*}
  J=\begin{pmatrix}
    \lambda_1&\delta_1&0&\dots&0&0\\
    0&\lambda_2&\delta_2&\dots&0&0\\
    0&0&\lambda_3&\dots&0&0\\
    &&&\dots&&\\
    0&0&0&\dots&\lambda_{n-1}&\delta_{n-1}\\
    0&0&0&\dots&0&\lambda_n\\
  \end{pmatrix}
\end{equation*}
Die $\lambda_i$ sind die Eigenwerte der Matrix $A$ und die $\delta_i$ entweder
$0$ oder $1$. Es gibt eine Transformationsmatrix $C$, die $A$ auf die
Jordanform gemäß \[ C^{-1}\cdot A\cdot C=J \] abbildet. Für einen vielfachen
Eigenwert $\lambda_k$ mit der Multiplizität $k$ bilden wir aus den Lösungen der
Gleichungen $(A - \lambda_k \mathbbm{1})^2 \vec{c}^T = 0$, $(A - \lambda_k
\mathbbm{1})^3\vec{c}^T = 0$, \dots, $(A - \lambda_k \mathbbm{1})^k \vec{c}^T =
0$ linear unabhängige Vektoren, die wir zur Transformationsmatrix
zusammenbauen. Wir bringen das System (\ref{eq:System1stOrder}) auf die Jordanform und lösen dann die Gleichung
\begin{equation}
  \begin{pmatrix}\dot{z}_1\\\dot{z}_2\\\dot{z}_3\\\dots\\\dot{z}_{n-1}\\\dot{z}_{n}\end{pmatrix}=
  \begin{pmatrix}
    \lambda_1&\delta_1&0&\dots&0&0\\
    0&\lambda_2&\delta_2&\dots&0&0\\
    0&0&\lambda_3&\dots&0&0\\
    &&&\dots&&\\
    0&0&0&\dots&\lambda_{n-1}&\delta_{n-1}\\
    0&0&0&\dots&0&\lambda_n\\
  \end{pmatrix}
  \begin{pmatrix}z_1\\z_2\\z_3\\\dots\\z_{n-1}\\z_{n}\end{pmatrix}
  \label{eq:JordanNormalForm} 
\end{equation}
Wieder lösen wir das System (\ref{eq:JordanNormalForm}) nach $\vec{z}^T$ auf
und transformieren danach wieder zurück\newline $\vec{y}^T=C\cdot\vec{z}^T$. 
\subsection{Inhomogene Systeme erster Ordnung}
In diesem Abschnitt beschäftigen wir uns mit den inhomogenen Systemen von
Differentialgleichungen erster Ordnung der Form
\begin{equation}
  \dot{\vec{\mathbf{x}}}=\mathbf{A}\vec{\mathbf{x}}+\vec{\mathbf{b}}
  \label{eq:InhomSystem}
\end{equation}
Eine Fundamentallösung von (\ref{eq:InhomSystem}) in der Form einer
Matrixfunktion $\boldsymbol{\Phi}(t)$ gehorcht der Gleichung 
\begin{equation}
  \dot{\boldsymbol{\Phi}}(t)=\mathbf{A}\boldsymbol{\Phi}(t)
  \label{eq:Fundamentalmatrix}
\end{equation}
$\boldsymbol{\Phi}(t)=e^{At}$ ist eine Fundamentalmatrix, welche die Bedingung
$\boldsymbol{\Phi}(0)=\mathbbm{1}$ erfüllt. Für (\ref{eq:InhomSystem}) haben wir
$\boldsymbol{\Phi}(t)=e^{At}\mathbf{C}$, mit einer beliebigen nichtsingulären
Matrix $\mathbf{C}$.  Wenn aber mit $\boldsymbol{\Phi(t)}$ eine Fundamentalmatrix
gefunden ist, dann ist es leicht ein System wie (\ref{eq:InhomSystem}) mit den
Anfangsbedingungen $\mathbf{x}(0)=\mathbf{x}_0$ zu lösen
\begin{equation}
  \mathbf{x}(t)=\boldsymbol{\Phi}(t)\boldsymbol{\Phi}^{-1}(0)\mathbf{x}_0
  +\int\limits_0^t\boldsymbol{\Phi}(t)\boldsymbol{\Phi}^{-1}(\tau)\vec{\mathbf{b}}(\tau)d\tau
  \label{eq:LoesInhom}
\end{equation}
Setzen wir hier $\boldsymbol{\Phi}(t)=e^{\mathbf{A}t}$ ein, dann erhalten wir
\[ \mathbf{x}(t)=e^{\mathbf{A}t}\mathbf{x}_0
   +e^{\mathbf{A}t}\int\limits_0^te^{-\mathbf{A}\tau}\;\vec{\mathbf{b}}(\tau)d\tau\]

   \begin{example}{Getriebener harmonischer Oszillator}
     \[\ddot{x}(t)+x(t)=f(t)\]
     kann als inhomogenes System erster Ordnung geschrieben werden. Wir setzen
     $y_1(t)=x(t)$ und $y_2(t)=\dot{x}(t)$
     \begin{align*}
       \dot{y}_1(t)&=y_2(t)\\
       \dot{y}_2(t)&=-y_1(t)+f(t)
     \end{align*}
     In diesem Fall ist
     \[\mathbf{A}=
       \begin{pmatrix}
	 0&1\\ -1&0
       \end{pmatrix}\quad
       \mbox{ und }\quad
       \vec{\mathbf{b}}(t)=\begin{pmatrix}0\\ f(t)\end{pmatrix}
     \]
     Wir wissen, dass
     \[e^{\mathbf{A}t}=
       \begin{pmatrix}
	 \cos(t)&-\sin(t)\\ \sin(t)&\cos(t)
       \end{pmatrix}
       =R(t)
     \]
     eine Rotationsmatrix ist und es ist
     \[e^{-\mathbf{A}t}=
       \begin{pmatrix}
         \cos(t)&\sin(t)\\-\sin(t)&\cos(t)
       \end{pmatrix}
       =R(-t)
     \]
     Wir erhalten als Lösung
     \[\mathbf{y}(t)=R(t)\mathbf{y}_0 + 
       R(t)\int\limits_0^t\begin{pmatrix}f(\tau)\sin(\tau)\\f(\tau)\cos(\tau)\end{pmatrix}d\tau
     \]
     Das ursprüngliche Oszillatorsystem hat also die Lösung $x(t)=y_1(t)$
     \[ x(t)=x_0\cos(t)-v_0\sin(t)+\int\limits_0^tf(\tau)\sin(\tau -t)d\tau \]
   \end{example}
\subsection{Randwertprobleme}
Gegeben eine lineare Differentialgleichung mit konstanten Koeffizienten 2. Ordnung
\begin{equation}
    \frac{d^2y(x)}{dx^2}+a\frac{dy(x)}{dx}+by(x)=g(x)
    \label{eq:LinDGL2Ordnung}
\end{equation}
Wir machen einen Exponentialansatz $y(x)=e^{\lambda x}$ und erhalten für die homogene Differentialgleichung (\ref{eq:LinDGL2Ordnung})
\begin{equation}
    \lambda^2+a\lambda+b=0
    \label{eq:CharGL}
\end{equation}
als charackteristische Gleichung. Hat (\ref{eq:CharGL}) zwei Lösungen, so ergibt dies auch zwei Lösungen $y_1(x)=e^\lambda_1 x$ und $y_2(x)=e^\lambda_2x$. Die zwei Lösungen überlagern wir um eine Lösung von (\ref{eq:LinDGL2Ordnung}) zu erhalten
\[ y(x)=c_1 y_1(x)+c_2 y_2(x) \]

Nun sei $g(x)=0$ und die beiden Randbedingungen $y(0)=\alpha$ und $y(L)=\beta$ seien gegeben. Damit erhalten wir foldendes Gleichungssystem, das wir erfüllen müssen
\begin{equation}
    \begin{pmatrix}y_1(0)&y_2(0)\\ y_1(L)&y_2(L)\end{pmatrix}
    \begin{pmatrix}c_1\\ c_2\end{pmatrix} = 
     \begin{pmatrix}\alpha \\ \beta\end{pmatrix}
\label{eq:Randbedingungen}
\end{equation}
Aus (\ref{eq:Randbedingungen}) berechnen wir die Lösungen für $c_1$ und $c_2$, wobei diese Gleichung nicht notwendigerweise eine Lösung hat, dies ist zuerst zu zeigen. Sollte (\ref{eq:Randbedingungen}) nicht lösbar sein, so haben wir Randbedingungen gefordert, die durch das Gleichungssysten nicht efüllt werden können.

In Falle $g\ne 0$ gehen wir vor, wie wir es bei den Anfangswertproblemen auch getan haben. Wir überlagern die homogene und die partikuläre Lösung $y(x)=y_h(x)+y_p(x)$ und bekommen ein Gleichungssystem, das nun auch die Funktion $g(x)$ enthält, nämlich durch (\ref{eq:DGL2OrdnungSolu}) eben in der Form
\[ y_p(x)=e^{\lambda_2 x}\int\limits_0^x e^{(\lambda1-\lambda_2)\eta}\int\limits_0^{\eta}e^{-\lambda_1 \xi}g(\xi)d\xi d\eta\]
Damit wird (\ref{eq:Randbedingungen}) zu
\begin{equation}
    \begin{pmatrix}y_1(0)&y_2(0)\\ y_1(L)&y_2(L)\end{pmatrix}
    \begin{pmatrix}c_1\\ c_2\end{pmatrix} = 
     \begin{pmatrix}\alpha+y_p(0) \\ \beta+y_p(L)\end{pmatrix}
\label{eq:RandBDInhom}
\end{equation}
\newpage
